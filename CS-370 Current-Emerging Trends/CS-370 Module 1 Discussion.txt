One of the readings we had this week had a great example of bias in an application. Facebook bases what comes into your News Feed off of what we have previously clicked and what our friends like (Granados). This is not an inherently bad thing because it should make it so it is populated with articles that you should find interesting. As long as you are aware of this you can also go to other places for information since you know that it is biased. I know there are plenty of times I have accidentally clicked on an article and started getting tons of similar articles popping up, and it is next to impossible to get them to stop. Another examble is Chatbot Tay from Microsoft. This chatbot was designed to learn from conversations with users on the app, and “relevant public data” would be “modeled, cleaned and filtered” (Denison). Naturally this didn't happen and within 1 day the chatbot was sharing tweets that were racist, transphobic and antisemitic.
There are lots of ways that you can try to reduce bias in AI. One is to go through your training data and make it more representative of what you want to accomplish. Another is to use a lot of diversity in labeling the data in the first place, so everyone is represented in it. Listening to user feedback is another strategy that can help identify if there is a bias in the application. I think the real challenge is to enter data from a broken world into a system that will just learn the biases that are already present. In the case of the Tay chatbot, how do you fix that? Make everyone be nice? We want AI to act human and then are dissapointed when it does. If you have to go in and filter the training data and clean it and make it exactly how you want it, why not just hardcode what you want instead of messing around with AI? One line in all the readings really stuck with me - “Garbage in means Garbage out” (Xiang).


Denison, George. 4 Shocking AI Bias Examples. October 2023. https://www.prolific.com/blog/shocking-ai-bias

Granados, Nelson. How Facebook Biases Your News Feed. June 2016. https://www.forbes.com/sites/nelsongranados/2016/06/30/how-facebook-biases-your-news-feed/?sh=26bda6321d51

Xiang, Mark. Human Bias in Machine Learning. March 2019. https://towardsdatascience.com/bias-what-it-means-in-the-big-data-world-6e64893e92a1